{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os, glob\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    return unicode(''.join([i if ord(i) < 128 else ' ' for i in text]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    doc = clean(text)\n",
    "\n",
    "    doc = str(doc).lower()\n",
    "    doc = nlp(clean(doc))\n",
    "    \n",
    "    # we add some words to the stop word list\n",
    "    texts, article = '', []\n",
    "    for w in doc:\n",
    "        if str(w) != '\\n':\n",
    "            texts = texts + ' ' + str(w.text)\n",
    "\n",
    "\n",
    "    i = 0\n",
    "    while i < len(texts):\n",
    "        if((ord(texts[i])>=65 and ord(texts[i]) <=90) or ord(texts[i]) == 46):\n",
    "            pass\n",
    "        elif((ord(texts[i])>=97 and ord(texts[i]) <=122) or ord(texts[i]) ==95 or ord(texts[i]) == 32):\n",
    "            pass\n",
    "        else:\n",
    "            texts = texts.replace(texts[i], '')\n",
    "        i +=1\n",
    "            #texts.remove(texts[i])\n",
    "            \n",
    "    return texts\n",
    "\n",
    "\n",
    "def noun_adj(texts):\n",
    "\n",
    "    tokens = nltk.sent_tokenize(texts)\n",
    "    #print len(tokens)\n",
    "    keyphrases = []\n",
    "    for i in range(len(tokens)):\n",
    "        tok = nltk.word_tokenize(tokens[i])\n",
    "        #print tok\n",
    "        pos = nltk.pos_tag(tok)\n",
    "        #print pos\n",
    "        #print pos\n",
    "        l, ll = '', []\n",
    "        for word, pos in pos:\n",
    "            if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS' or pos == 'JJ' or pos == 'JJR' or pos == 'JJS'):\n",
    "                if len(l) != 0:\n",
    "                    l = l + \" \" + word\n",
    "                else:\n",
    "                    l = word\n",
    "            elif len(l) != 0:\n",
    "                ll.append(l)\n",
    "                l = ''\n",
    "        if len(l) != 0:\n",
    "            ll.append(l)\n",
    "            l = ''\n",
    "        keyphrases.append(ll)\n",
    "        ll = []\n",
    "\n",
    "    return keyphrases\n",
    "\n",
    "def mainKeyphrases(keyphrases):\n",
    "        #not w.is_stop and not w.is_punct and not w.like_num\n",
    "    #print ll\n",
    "    #print nltk.word_tokenize(ll[9])\n",
    "    mainKeys, ke, kk = [], [], []\n",
    "    #for i in range(len(keyphrases)):\n",
    "    i=0\n",
    "    while i < len(keyphrases):\n",
    "        #############\n",
    "        if len(keyphrases[i]) == 0:\n",
    "            del keyphrases[i]\n",
    "            i -= 1\n",
    "        else:\n",
    "            for j in range(len(keyphrases[i])):\n",
    "                    #print i,j\n",
    "                    #print keyphrases[i][j]\n",
    "                kk =  nltk.word_tokenize(keyphrases[i][j])\n",
    "                    #print kk\n",
    "                k=0\n",
    "                while k < len(kk):\n",
    "                    ####################3\n",
    "                    if len(kk[k]) < 3:\n",
    "                        del kk[k]\n",
    "                        k -= 1\n",
    "                    k+=1\n",
    "                            \n",
    "                ke.append(kk)\n",
    "            mainKeys.extend(ke)\n",
    "            ke = []\n",
    "        i += 1\n",
    "    #print mainKeys\n",
    "    \n",
    "    #joining keyphrases\n",
    "    keyphrase = []\n",
    "    for i in mainKeys:\n",
    "        keyphrase.append(' '.join(i))\n",
    "        \n",
    "    #removing empty items\n",
    "    i=0\n",
    "    while i < len(keyphrase):\n",
    "        if len(keyphrase[i]) < 1:\n",
    "            keyphrase.remove(keyphrase[i])\n",
    "            i-=1\n",
    "        i+=1\n",
    "    \n",
    "    return keyphrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def keyphrase_extraction(text, n):\n",
    "    \n",
    "    vec = TfidfVectorizer()\n",
    "    tfidf_vec = vec.fit_transform((str(x) for x in text))\n",
    "    \n",
    "    ranking = list()\n",
    "    for i, c in enumerate(tfidf_vec):\n",
    "        ranking.append((i, c.sum()))\n",
    "        \n",
    "    top_sentences = sorted(ranking, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    result = []\n",
    "    for i in top_sentences:\n",
    "        result.append(text[i[0]])\n",
    "        \n",
    "    seen = set()\n",
    "    result = [x for x in result if not (x in seen or seen.add(x))]\n",
    "    return result[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['canadian pop star michael buble married argentine actress luisana lopilato', 'buble weds star luisana lopilato canadian pop star michael buble', 'north america top grossing concert entertainers', 'bride argentine actress luisana lopilato', 'rebel way little girls']\n"
     ]
    }
   ],
   "source": [
    "lee_train_file = \"art_and_culture.txt\"\n",
    "with open(lee_train_file) as f:\n",
    "    story = f.read()\n",
    "    texts = preprocessing(story)\n",
    "    filtered = noun_adj(texts)\n",
    "    phrases = mainKeyphrases(filtered)\n",
    "print keyphrase_extraction(phrases, 5)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
